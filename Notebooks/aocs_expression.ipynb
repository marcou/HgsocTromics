{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying ICA / NMF to AOCS Ovarian Cancer gene expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import NMF, FastICA, PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import statsmodels.formula.api as sm\n",
    "import pickle\n",
    "import mygene\n",
    "import qgrid\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from factorizer_wrappers import ICA_Factorizer, NMF_Factorizer, PCA_Factorizer, Nimfa_NMF_Factorizer\n",
    "from factorizer_wrappers import example_V, test_example_V, test_Factorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(10,6)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example_V()\n",
    "\n",
    "test_Factorizer(ICA_Factorizer(n_components=4), atol=0.5) \n",
    "test_Factorizer(ICA_Factorizer(n_components=5), atol=0.001) \n",
    "\n",
    "test_Factorizer(PCA_Factorizer(n_components=4), atol=0.5) \n",
    "test_Factorizer(PCA_Factorizer(n_components=5), atol=0.001) \n",
    "\n",
    "test_Factorizer(NMF_Factorizer(n_components=4), atol=0.5) \n",
    "test_Factorizer(NMF_Factorizer(n_components=5), atol=0.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix plotting utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_norm_diff(m1, m2):\n",
    "#   return np.mean(np.sqrt((m1 - m2)**2))\n",
    "    return np.sqrt(np.mean((m1 - m2)**2))\n",
    "\n",
    "def test_l2_norm_diff():\n",
    "    V = example_V(10)\n",
    "    rms = l2_norm_diff(V, V+0.5)\n",
    "    assert np.isclose(rms,0.5)\n",
    "    \n",
    "test_l2_norm_diff() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_W_H_WH_V(W, H, V, rec_V, n_genes_to_pick=None):\n",
    "    \"\"\" Show factorization matrices in visually pleasing form\"\"\"\n",
    "    \n",
    "    if n_genes_to_pick is None:\n",
    "        gene_ixs = range(V.shape[0])\n",
    "        title = \"Matrix decomposition, showing all geges\"\n",
    "    else:\n",
    "        gene_ixs = sorted(np.random.randint(0, V.shape[0], n_genes_to_pick))\n",
    "        title = \"Matrix decomposition, randomly selecting %d genes for visibility\" % n_genes_to_pick\n",
    "    fig, axs = plt.subplots(1,4, figsize=(17,6))\n",
    "    fig.suptitle(title, size=16)\n",
    "    axs[0].imshow(W[gene_ixs,:], aspect='auto')\n",
    "    axs[0].set_title('W')\n",
    "    axs[0].set_ylabel('genes', size=14)\n",
    "    axs[0].set_xlabel('factors', size=14)\n",
    "    \n",
    "    axs[0].set_xticklabels('')\n",
    "    \n",
    "    axs[1].imshow(H, aspect='auto')\n",
    "    axs[1].set_title('H')\n",
    "    axs[1].set_ylabel('factors', size=14)\n",
    "    axs[1].set_xlabel('patients', size=14)\n",
    "    axs[1].set_yticklabels('')\n",
    "    \n",
    "    rms_err = l2_norm_diff(rec_V, V)\n",
    "    axs[2].imshow(rec_V[gene_ixs,:], aspect='auto')\n",
    "    axs[2].set_title('W H (RMS err=%6.2f)' % rms_err)\n",
    "   \n",
    "    \n",
    "    axs[3].imshow(V[gene_ixs,:], aspect='auto')\n",
    "    axs[3].set_title('V')\n",
    "    axs[3].set_ylabel('genes', size=14)\n",
    "    axs[3].set_xlabel('patients', size=14)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def test_show_W_H_WH_V():\n",
    "    \n",
    "    V = example_V(10)\n",
    "    print(\"v.shape\", V.shape)\n",
    "    facto = ICA_Factorizer(n_components=2)\n",
    "    facto.fit(V)\n",
    "    W = facto.get_W()\n",
    "    H = facto.get_H()\n",
    "    print(\"H.shape\", H.shape)\n",
    "    \n",
    "    show_W_H_WH_V(W,H, V, facto.get_recovered_V())\n",
    "    show_W_H_WH_V(W,H,V, facto.get_recovered_V(), 100)\n",
    "test_show_W_H_WH_V()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and explore the expression matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in expression spreadsheet which has been processed (see end of notebook) to inlcude only protein coding genes\n",
    "expression_df = pd.read_csv('../Data/HGSOC_Protein_Expression.csv', sep='\\t')\n",
    "expression_df.set_index('GeneENSG', inplace=True)\n",
    "assert len(expression_df) == 19730   # Only \n",
    "assert len(expression_df.columns) == 80\n",
    "assert expression_df.columns[-1] == 'AOCS_171'\n",
    "\n",
    "expression_matrix = np.asarray(expression_df)\n",
    "\n",
    "print(expression_matrix.shape[0], \"genes\")\n",
    "print(expression_matrix.shape[1], \"patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 12))\n",
    "plt.imshow(expression_matrix, aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.xlabel((\"Patients\"))\n",
    "plt.ylabel((\"Genes\"))\n",
    "plt.title(\"Expression matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a dictionary to map Ensembl ENSG ids to symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is run-once code to query for all the Ensemble gene IDs we're using, construct a dictionary and write\n",
    "# it to file.\n",
    "\n",
    "ensgDictFile = '../Cache/ensgDict.pkl'\n",
    "if not path.exists(ensgDictFile):  # Run only if dictionary file does not already exist\n",
    "    mg = mygene.MyGeneInfo()\n",
    "    ensgIDs = expression_df.index.values.tolist()    # All the gene IDs in this study\n",
    "    ginfo = mg.querymany(ensgIDs, scopes='ensembl.gene')\n",
    "\n",
    "    ensgDict = {}\n",
    "    for g in ginfo:\n",
    "        ensg = g['query']\n",
    "        del g['query'] \n",
    "        ensgDict[ensg] = g\n",
    "\n",
    "    print(\"Writing to %s...\" % ensgDictFile)\n",
    "    with open(ensgDictFile, 'wb') as f:\n",
    "        pickle.dump(ensgDict, f)\n",
    "    print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the gene dictionary file\n",
    "with open(ensgDictFile, 'rb') as f:\n",
    "    ensgDict = pickle.load(f)\n",
    "    \n",
    "for (ensg, g) in ensgDict.items():\n",
    "    if 'symbol' not in g.keys():\n",
    "        g['symbol'] = ensg    # ensure lookup always succeeds\n",
    "    \n",
    "# Example use:\n",
    "def example_ensgDict_use():\n",
    "    gid = 'ENSG00000000938'\n",
    "    # All ENSG ids used in this study should be in the dictionary\n",
    "    ginfo = ensgDict[gid]\n",
    "              \n",
    "example_ensgDict_use()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note prior normalisation of the expression array\n",
    "Normalisation was applied by Ailith's script, using the method of a varaince stabalising transform.  See below, all patients have a minimum of aproximately 3.5, maximum approaximately 23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "expression_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot distributions of expression data\n",
    "... for a quick visual check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_expression_distributions(V):\n",
    "    def labeled_figure():\n",
    "        plt.figure(figsize=(14,4))\n",
    "        plt.xlabel('Expression level')\n",
    "        plt.ylabel('Frequency')\n",
    "        \n",
    "    if True:\n",
    "        labeled_figure()\n",
    "        _ = plt.hist(V.ravel(), bins=40)\n",
    "        plt.title(\"Distribution of all normalised expression levels\")\n",
    "        plt.show()\n",
    "    \n",
    "    if True:\n",
    "        labeled_figure()\n",
    "        _ = plt.hist(V, bins=40)\n",
    "        plt.title(\"Distribution of normalised expression levels, broken out by patient\")\n",
    "        plt.show()\n",
    "        \n",
    "    if True:\n",
    "        labeled_figure()\n",
    "        n_genes_to_pick = 100\n",
    "        random_gene_ixs = sorted(np.random.randint(0, V.shape[0], n_genes_to_pick))\n",
    "        _ = plt.hist(V[random_gene_ixs,:].T, bins=10)\n",
    "        plt.title(\"Distribution of normalised expression levels, broken out by gene, for random %d genes\" %\n",
    "                  n_genes_to_pick)\n",
    "        plt.show()\n",
    "    \n",
    "show_expression_distributions(expression_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the patient metadata\n",
    "In particular we are interested in treatment \"Resposnse\", which we scraped from the Patch paper (code at end of notebool)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read metadata (which we scraped from the Patch etal paper!)\n",
    "metadata_df = pd.read_csv('../Data/AOCS_metadata.csv', index_col='AOCS_ID')\n",
    "assert metadata_df.columns[0] == \"Age\"\n",
    "assert metadata_df.columns[1] == \"Response\"\n",
    "# Make sure the IDs match-up between the two dataframes\n",
    "assert (all(metadata_df.index == expression_df.columns))\n",
    "metadata_df['Response'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qgrid.show_grid(metadata_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use ICA, NMF and PCA factorization and plot stuff..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def fit_and_plot_model(V, met_df, facto, plot=True):\n",
    "\n",
    "    facto.fit(V)\n",
    "    W = facto.get_W()\n",
    "    H = facto.get_H()\n",
    "\n",
    "    # Show the factored matrices and compare the reconstruction with the original\n",
    "    if plot:\n",
    "        show_W_H_WH_V(W, H, V, facto.get_recovered_V(), n_genes_to_pick=200)\n",
    "    \n",
    "    plot_df = metadata_df.copy().drop('Age', axis=1)\n",
    "    \n",
    "    factors = ['Factor_%d'%i for i in range(facto.n_components)]\n",
    "    for i in range(len(factors)):\n",
    "        plot_df[factors[i]] = H[i, :]\n",
    "    \n",
    "    # Boxplots of H factors by Response\n",
    "    if plot:\n",
    "        plot_df.boxplot(column=factors, by='Response', fontsize=10, figsize=(14,4), layout=(1, facto.n_components))\n",
    "        plt.show()    \n",
    "\n",
    "    # Scatter plots of metagenes matrix - W - using Seaborne\n",
    "    if plot:\n",
    "        sns.pairplot(plot_df, hue='Response')\n",
    "        plt.show()\n",
    "        \n",
    "    # Make a t-SNE plot\n",
    "    if plot:\n",
    "        tsne = TSNE(n_components=2, init='pca', random_state=42, n_jobs=7)\n",
    "        Y = tsne.fit_transform(W)\n",
    "        sns.scatterplot(Y[:,0], Y[:,1])\n",
    "        plt.show()\n",
    "    \n",
    "    # Put together a dictionary or results\n",
    "    \n",
    "    results_dict = {}\n",
    "    \n",
    "    # Find factor which best explains response\n",
    "    \n",
    "    ols_results = [sm.ols(fact + '~ C(Response)', data=plot_df).fit() for fact in factors]\n",
    "    rsqs = [res.rsquared for res in ols_results]\n",
    "    results_dict['best_rsq'] = np.max(rsqs)\n",
    "    results_dict['best_factor'] = np.argmax(rsqs)\n",
    "    results_dict['rms_err'] = l2_norm_diff(V, facto.get_recovered_V())\n",
    "    \n",
    "    return results_dict\n",
    "\n",
    "print(\"================== ICA ======================\")\n",
    "result = fit_and_plot_model(expression_matrix, metadata_df,     \n",
    "                            ICA_Factorizer(n_components=14),\n",
    "                            plot=False)\n",
    "\n",
    "print(\"\\n================== NMF ======================\")\n",
    "result = fit_and_plot_model(expression_matrix, metadata_df,     \n",
    "                            NMF_Factorizer(n_components=14),\n",
    "                            plot=False)\n",
    "\n",
    "print(\"\\n================== PCA ======================\")\n",
    "result = fit_and_plot_model(expression_matrix, metadata_df,     \n",
    "                            PCA_Factorizer(n_components=14),\n",
    "                            plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    facto=ICA_Factorizer(n_components=14)\n",
    "    V = expression_matrix\n",
    "    facto.fit(V)\n",
    "    W = facto.get_W()\n",
    "    H = facto.get_H()\n",
    "\n",
    "    # Make a t-SNE plot\n",
    "    tsne = TSNE(n_components=2, init='pca', random_state=42, n_jobs=7)\n",
    "    Y = tsne.fit_transform(W)\n",
    "    \n",
    "    #response_dict = {'Sensitive':0, 'Refractory':1, 'Resistant':2}\n",
    "    #colour = np.array([response_dict[r] for r in plot_df['Response']]).T\n",
    "    sns.scatterplot(Y[:,0], Y[:,1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def retreive_or_generate_results1_df():\n",
    "    # Explore results for ICA, NMP and PCA, generating a list of dictionaries\n",
    "    resultsFile = '../Cache/results1.csv'\n",
    "    if not path.exists(resultsFile):\n",
    "        Factos = [ICA_Factorizer, NMF_Factorizer, PCA_Factorizer]\n",
    "        results1 = []\n",
    "        for nc in range(2,40,2):\n",
    "            for random_state in [42, 345, 13, 235, 583]:\n",
    "                for Facto in Factos:\n",
    "                    params = {'n_components':nc, 'random_state':random_state}\n",
    "\n",
    "                    facto = Facto(**params)\n",
    "                    params['which'] = type(facto).__name__\n",
    "                    print(params)\n",
    "                    res = fit_and_plot_model(expression_matrix, metadata_df, facto, plot=False)\n",
    "                    print(res)\n",
    "                    results1.append({**params, **res})\n",
    "\n",
    "        print(\"Writing results1.csv\")\n",
    "        results1_df = pd.DataFrame(results1)\n",
    "        results1_df.to_csv('results1.csv')\n",
    "        print(\"Done.\")\n",
    "\n",
    "    print(\"Reading\", resultsFile)\n",
    "    results1_df = pd.read_csv(resultsFile)\n",
    "    results1_df = results1_df.drop(columns=['Unnamed: 0'])\n",
    "    return results1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results1_df = retreive_or_generate_results1_df()\n",
    "qgrid.show_grid(results1_df)\n",
    "results1_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rms_err vs components for each method\n",
    "results1_df = retreive_or_generate_results1_df()\n",
    "for which in [\"ICA_Factorizer\", \"NMF_Factorizer\", \"PCA_Factorizer\"]:\n",
    "    which_df = results1_df[results1_df['which']==which]\n",
    "    x,y  = which_df['n_components'], which_df['rms_err']\n",
    "    plt.plot(x,y, label=which[:3])\n",
    "plt.legend()\n",
    "plt.xlabel(\"n_components\")\n",
    "plt.ylabel(\"rms_err\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot best_rsq fit to response vs components for each method\n",
    "results1_df = retreive_or_generate_results1_df()\n",
    "for which in [\"ICA_Factorizer\", \"NMF_Factorizer\", \"PCA_Factorizer\"]:\n",
    "    which_df = results1_df[results1_df['which']==which]\n",
    "    which_df = which_df.groupby('n_components').mean()\n",
    "    x,y  = which_df.index, which_df['best_rsq']\n",
    "    plt.plot(x,y, label=which[:3])\n",
    "plt.legend()\n",
    "plt.xlabel(\"n_components\")\n",
    "plt.ylabel(\"best_rsq\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_df = results1_df[results1_df['which']==which]\n",
    "which_df.groupby('n_components').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_or_generate_results2_df():\n",
    "# Explore FastICA with 14 components, for various parameters\n",
    "    resultsFile = '../Cache/results2.csv'\n",
    "    if not path.exists(resultsFile):\n",
    "        results2 = []\n",
    "        nc = 14\n",
    "        for random_state in [42, 13, 56]:\n",
    "            for max_iter in range(1, 100, 5):\n",
    "                for fun in ['logcosh'] :# , 'exp', 'cube':\n",
    "                    params = {'n_components':nc, 'random_state':random_state,\n",
    "                              'fun':fun, 'max_iter':max_iter}\n",
    "                    print(params)\n",
    "                    facto = ICA_Factorizer(**params)\n",
    "                    res = fit_and_plot_model(expression_matrix, metadata_df, facto, plot=False)\n",
    "                    print(res)\n",
    "                    results2.append({**params, **res})\n",
    "\n",
    "        print(\"Writing results2.csv\")\n",
    "        results2_df = pd.DataFrame(results2)\n",
    "        results2_df.to_csv('results2.csv')\n",
    "        print(\"Done.\")\n",
    "\n",
    "    print(\"Reading\",resultsFile)\n",
    "    results2_df = pd.read_csv(resultsFile)\n",
    "    return results2_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qgrid.show_grid(retreive_or_generate_results2_df())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring distribution of weights in W and H matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrix_weight_distributions(facto):\n",
    "    facto.fit(expression_matrix)\n",
    "    W = facto.get_W()\n",
    "    H = facto.get_H()\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.suptitle(\"Distribution of W and H matrix weights for %s\" %type(facto).__name__, size=16)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.hist(W.ravel(), bins=50, log=True)\n",
    "    plt.xlabel(\"W matrix weights\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.hist(H.ravel(), bins=20, log=True)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xlabel(\"H matrix weights\")\n",
    "    plt.show()\n",
    "    \n",
    "facto = ICA_Factorizer(n_components=14)\n",
    "plot_matrix_weight_distributions(facto)\n",
    "\n",
    "facto = NMF_Factorizer(n_components=14)\n",
    "plot_matrix_weight_distributions(facto)\n",
    "\n",
    "facto = PCA_Factorizer(n_components=14)\n",
    "plot_matrix_weight_distributions(facto)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's find some influential genes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the k=14 metagenes matrix found by BIODICA\n",
    "\n",
    "biodica_matrix_file = \"../Factors/S_HGSOC_Protein_Expression_ica_numerical.txt_6.num\"\n",
    "biod_ica = np.loadtxt(biodica_matrix_file)\n",
    "biod_ica.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "all_genes = expression_matrix.shape[0]\n",
    "n_genes = all_genes   # trim for speed while we develop\n",
    "n_comps = 6\n",
    "expression_matrix[:n_genes,:]\n",
    "\n",
    "gene_ENSG_ids = expression_df.index.values[:n_genes]\n",
    "gene_symbols = [ensgDict[ensg]['symbol'] for ensg in gene_ENSG_ids]\n",
    "nmf_facto = NMF_Factorizer(n_components=n_comps, max_iter=1000)\n",
    "ica_facto = ICA_Factorizer(n_components=n_comps)\n",
    "pca_facto = PCA_Factorizer(n_components=n_comps)\n",
    "\n",
    "V = expression_matrix\n",
    "nmf_facto.fit(V)\n",
    "print(\"NMF fit done\")\n",
    "ica_facto.fit(V)\n",
    "print(\"ICA fit done\")\n",
    "pca_facto.fit(V)\n",
    "print(\"PCA fit done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_component_ranked_plots(metagene_matrix):\n",
    "    assert metagene_matrix.shape == (n_genes, n_comps)\n",
    "    fig = plt.figure(figsize=(14,14))\n",
    "\n",
    "    for ci in range(n_comps):\n",
    "        plt.subplot(4,4,ci+1)\n",
    "        metagene = metagene_matrix[:, ci]\n",
    "        #influence = abs(metagene)\n",
    "        influence = metagene\n",
    "        gixpairs = zip(gene_symbols, influence)\n",
    "        gixpairs = sorted(gixpairs, key=lambda p: -p[1])\n",
    "        ranked_symbols, ranked_influence,  = zip(*gixpairs)\n",
    "        plt.plot(ranked_influence)\n",
    "        plt.yscale('log')\n",
    "        if ci == 0:\n",
    "            plt.xlabel('Influence rank')\n",
    "            plt.ylabel('Influence')\n",
    "        fig.tight_layout() \n",
    "        plt.title('Component %d' % ci)\n",
    "    plt.show()    \n",
    "\n",
    "\n",
    "show_component_ranked_plots(abs(biod_ica))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_component_distributions(metagene_matrix):\n",
    "    assert metagene_matrix.shape == (n_genes, n_comps)\n",
    "    fig = plt.figure(figsize=(14,14))\n",
    "\n",
    "    for ci in range(n_comps):\n",
    "        plt.subplot(4,4,ci+1)\n",
    "        metagene = metagene_matrix[:, ci]\n",
    "        #influence = abs(metagene)\n",
    "        influence = metagene\n",
    "        \n",
    "        plt.hist(influence, bins=20)\n",
    "        # plt.yscale('log')\n",
    "        if ci == 0:\n",
    "            plt.xlabel('Influence')\n",
    "            plt.ylabel('Freq')\n",
    "        fig.tight_layout() \n",
    "        plt.title('Component %d' % ci)\n",
    "    plt.show()    \n",
    "\n",
    "show_component_distributions(biod_ica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse standard deviation of components\n",
    "n_stddev = 3.0\n",
    "for ci in range(biod_ica.shape[1]):\n",
    "    metagene = biod_ica[:,ci]\n",
    "    stddev = np.std(metagene)\n",
    "    threshold = n_stddev * stddev\n",
    "    num_above_threshold = len(metagene[abs(metagene) > threshold])\n",
    "    print(\"Component %d, SD=%4.2f, #genes outside %3.1f SDs=%d\" % (ci, stddev, n_stddev, num_above_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def select_influential_genes(metagene, top_n):\n",
    "    influence = abs(metagene)\n",
    "    stddev = np.std(metagene)\n",
    "    mean = np.mean(metagene)\n",
    "    assert abs(mean) < 0.01    # we'll need to do something else for +ve NMF\n",
    "    threshold = n_stddev * stddev\n",
    "    gixpairs = zip(gene_symbols, influence)\n",
    "    selection = [symbol for (symbol, v) in gixpairs if abs(v) > threshold]\n",
    "    \n",
    "    return selection    \n",
    "\n",
    "W = biod_ica\n",
    "ranked_genes_by_component = {}\n",
    "for ci in range(n_comps):\n",
    "    _genes = select_influential_genes(W[:,ci], 100)\n",
    "    ranked_genes_by_component[ci] = _genes\n",
    "    if True:\n",
    "        print(\"Comp. %d: %s\" % (ci, ' '.join(_genes)))\n",
    "    else:\n",
    "        print(\"Comp. %d: \\n%s\\n\" % (ci, '\\n'.join(_genes)))\n",
    "        # print(\"Comp. %d: \\n%s\\n\" % (ci, genes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gene enrichment analysis using GOATOOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from goatools import obo_parser\n",
    "from goatools.go_enrichment import GOEnrichmentStudy\n",
    "import Bio.UniProt.GOA as GOA\n",
    "import gzip\n",
    "\n",
    "# Load the Gene Ontology\n",
    "gene_ontology = go = obo_parser.GODag('../DownloadedResources/go-basic.obo')\n",
    "\n",
    "# Load the human annotations\n",
    "c=0\n",
    "with gzip.open('../DownloadedResources/goa_human.gaf.gz', 'rt') as gaf:\n",
    "    funcs = {}\n",
    "    for entry in GOA.gafiterator(gaf):\n",
    "        c +=1 \n",
    "        uniprot_id = entry.pop('DB_Object_Symbol')\n",
    "        funcs[uniprot_id] = entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our population is the set of genes we are analysing\n",
    "\n",
    "population = [ensgDict[ensg]['symbol'] for ensg in expression_df.index.values]\n",
    "print(\"We have %d genes in our population\" % len(population))\n",
    "\n",
    "# Build associations from functional annotations we got from the gaf file\n",
    "associations = {}\n",
    "for x in funcs:\n",
    "    if x not in associations:\n",
    "        associations[x] = set()\n",
    "    associations[x].add(str(funcs[x]['GO_ID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gea = GOEnrichmentStudy(population, associations, gene_ontology,\n",
    "                         propagate_counts=True,\n",
    "                         alpha=0.05,\n",
    "                         methods=['fdr'])\n",
    "gea_results_by_component = {}\n",
    "for ci in range(n_comps):\n",
    "    study_genes = ranked_genes_by_component[ci]\n",
    "    gea_results_by_component[ci] = gea.run_study(study_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results into a dataframe per component.  Easiest way is to use routine to write a .tsv file, \n",
    "# then read back and filter\n",
    "\n",
    "gea_results_df_by_component = []\n",
    "for ci in range(n_comps):\n",
    "    tsv_name = '../Cache/goa_results_C%d.tsv' % ci\n",
    "    with open(tsv_name, 'w') as f:\n",
    "        gea.prt_tsv(f, gea_results_by_component[ci])\n",
    "    ge_df = pd.read_csv(tsv_name, sep='\\t')\n",
    "\n",
    "    ge_df.rename(columns={'# GO':'GO_ID'}, inplace=True)\n",
    "    ge_df.set_index('GO_ID', inplace=True)\n",
    "    ge_df.drop(columns=['NS', 'enrichment', 'p_uncorrected'], inplace=True)\n",
    "    ge_df = ge_df[ge_df['p_fdr'] <= 0.05]\n",
    "    ge_df['Component'] = ci\n",
    "    \n",
    "    gea_results_df_by_component += [ge_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the per-component dataframes into a single one\n",
    "gea_all_sig_results_df = pd.DataFrame()\n",
    "gea_all_sig_results_df = gea_all_sig_results_df.append(gea_results_df_by_component)\n",
    "gea_all_sig_results_df.to_csv('../Cache/gea_all_sig_results.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qgrid.show_grid(gea_all_sig_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run-once code following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following is run-once code for recovering metadata from the Patch et al paper..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract age and response group from scraped text file\n",
    "def extract_metadata_from_crazy_scraped_file(scrape_file):\n",
    "    with open(scrape_file, 'r') as f:\n",
    "        l1 = f.readline().strip()\n",
    "        l2 = f.readline().strip()\n",
    "    l1_words = l1.split(' ')\n",
    "    aocs_ids = l1_words[::2]\n",
    "    aocs_ids = [s.replace('-','_') for s in aocs_ids]\n",
    "    \n",
    "    ages = l1_words[1::2]\n",
    "    response = l2.split(' ')\n",
    "    assert len(aocs_ids) == len(ages) == len(response) == 80\n",
    "    # Build a dataframe\n",
    "    df = pd.DataFrame()\n",
    "    df['AOCS_ID'] = aocs_ids\n",
    "    df['Age'] = ages\n",
    "    df['Response'] = response\n",
    "    \n",
    "    df = df.set_index('AOCS_ID')\n",
    "    df = df.sort_index()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable only if 'AOCS_metadata.csv' is to be re-created\n",
    "if False:\n",
    "    metadata_df = extract_metadata_from_crazy_scraped_file('../Data/aocs_raw_figure_e6.txt')\n",
    "    metadata_df.to_csv('../Data/AOCS_metadata.csv')\n",
    "    readback_metadata_df = pd.read_csv('../Data/AOCS_metadata.csv', index_col='AOCS_ID')\n",
    "    readback_metadata_df\n",
    "    assert len(readback_metadata_df) == 80\n",
    "    readback_metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run-once code to convert create a protein coding gene only expression file\n",
    "\n",
    "The original 'AOCS_TPM_VST.csv' contains 57,424 transcripts, many of which are non-codeing.   We wish to work with protein coding genes only.  We proceed as follows:\n",
    "1. Read AOCS_TPM_VST.csv into a dataframe with ENSG identifiers as index\n",
    "1. Write a text file listing all ENSG identifiers extracted from AOCS_TPM_VST.csv creating ensg_list.txt\n",
    "1. Obtain an annotated gene table from [Biomart](https://m.ensembl.org/info/data/biomart/index.html):\n",
    "   1. Manually (should automate) upload ensg_list.txt\n",
    "   1. Select attributes of Gene stable ID, Gene name, Gene type, and Gene description; it's Gene type which is important\n",
    "   1. Export the generated table to 'DownloadedResources/mart_export.txt'\n",
    "1. Read mart_export.txt into a dataframe with ENSG identifiers as index and filter on Gene type == 'protein_coding'\n",
    "1. Merge the original full expression dataframe with the filtered dataframe\n",
    "1. Write out a tab-seperated file 'HGSOC_Protein_Expression.csv' containing GeneENSG as first column with patient expression values in the following 80 columns\n",
    "\n",
    "The generated HGSOC_Protein_Expression.csv is in a format suitable for direct input to BIODICA and can be used for all other analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Read in original full AOCS spreadsheet\n",
    "    full_expression_df = pd.read_csv('../Data/AOCS_TPM_VST.csv')\n",
    "    full_expression_df.set_index('GeneENSG', inplace=True)\n",
    "    assert len(full_expression_df) == 57914\n",
    "    assert len(full_expression_df.columns == 80 + 1)\n",
    "    assert full_expression_df.columns[-1] == 'AOCS_171'\n",
    "    ensglist = full_expression_df.index.values.tolist()\n",
    "    with open('../Cache/ensg_list.txt', 'w') as f:\n",
    "        f.write('\\n'.jTrueoin(ensglist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where you have to do the manual Biomart stuff as described above... then run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Read in the Biomart created file\n",
    "    mart_export_df = pd.read_csv('../DownloadedResources/mart_export.txt', sep='\\t')\n",
    "    mart_export_df.set_index('Gene stable ID', inplace=True)\n",
    "    assert mart_export_df.loc['ENSG00000198804', 'Gene type'] == 'protein_coding'\n",
    "    \n",
    "    # Create a dataframe containing only protein coding genes\n",
    "    mart_export_protein_coding_df = mart_export_df[mart_export_df['Gene type'] == 'protein_coding']\n",
    "\n",
    "    # Merge with full expression dataframe (only those present in both will be kept)\n",
    "    expression_protein_coding_df = pd.merge(\n",
    "        left=full_expression_df, right=mart_export_protein_coding_df, \n",
    "        left_index=True, right_index=True)\n",
    "    expression_protein_coding_df.drop(columns=['Gene name', 'Gene type', 'Gene description'], inplace=True)\n",
    "    assert len(expression_protein_coding_df.columns) == 80\n",
    "\n",
    "    # Write the filtered expression matrix to a .csv file\n",
    "    expression_protein_coding_df.to_csv('../Data/HGSOC_Protein_Expression.csv', index=True, index_label='GeneENSG', sep='\\t')\n",
    "    \n",
    "    # Read it back and check all in order\n",
    "    del expression_protein_coding_df\n",
    "    expression_protein_coding_df = pd.read_csv('../Data/HGSOC_Protein_Expression.csv', sep='\\t')\n",
    "    expression_protein_coding_df.set_index('GeneENSG', inplace=True)\n",
    "    assert len(expression_protein_coding_df.columns) == 80\n",
    "    assert len(expression_protein_coding_df) == 19730\n",
    "    \n",
    "    # Paranoia: the following specific expression value was manually extracted from the orginal AOCS_TPM_VST.csv,\n",
    "    # and is compared here to check we haven't somehow scrambled the ordering anywhere!\n",
    "    assert expression_protein_coding_df.loc['ENSG00000138772', 'AOCS_004'] == 12.6329098049671\n",
    "    \n",
    "    del full_expression_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(gea)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
